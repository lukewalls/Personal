{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue Data Science Code Challenge\n",
    "Luke Walls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-78cc8cab5216>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;31m# linear algebra\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/chat-data/chat_demand.csv', index_col=0, parse_dates=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Q1: Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data for the first half of 2018.\n",
    "Relatively small dataset with 181 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paid employees is significantly larger than chats. We will plot on two separate graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "# Visualize paid employees\n",
    "data_chats = data.drop(columns='paid_employees')\n",
    "data_paid = data.drop(columns='chats')\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title(\"Daily Paid Employees\")\n",
    "sns.lineplot(data=data_paid, legend=False)\n",
    "plt.xlabel(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize chats\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title(\"Daily Chats\")\n",
    "sns.lineplot(data=data_chats, legend=False)\n",
    "plt.xlabel(\"Date\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Chats gives time-series data; we can see an overall increasing trend and what looks to be constant cycles shown by the 'sinusoidal' pattern.\n",
    "\n",
    "There appears to be a repeating weekly pattern (a weekly cycle). This could indicate there are a greater number of chats on certain days of the week compared to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a difference in number of chats by days of the week? Are there more chats on weekdays or weekends?\n",
    "def expand_df(df):\n",
    "    data = df.copy()\n",
    "    data['day'] = data.index.day\n",
    "    data['month'] = data.index.month\n",
    "    data['dayofweek'] = data.index.dayofweek\n",
    "    return data\n",
    "\n",
    "data_expand = expand_df(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viz chats by day of the week\n",
    "\n",
    "data_expand_dow = data_expand.groupby('dayofweek').chats.mean()\n",
    "data_expand_dow.rename(index={0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Average Chats by Day of Week\")\n",
    "sns.barplot(x=data_expand_dow.index, y=data_expand_dow)\n",
    "plt.xlabel(\"Day of the Week\")\n",
    "plt.ylabel(\"Chats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of chats by day of the week\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Chats by Day of Week\")\n",
    "sns.boxplot(x='dayofweek', y='chats', data=data_expand)\n",
    "plt.xlabel(\"Day of the Week\")\n",
    "plt.ylabel(\"Chats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot(data=data_expand, x='month', y='chats', \n",
    "               col='dayofweek', palette='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thursday appears to be the busiest day of the week while Monday has fewer chats on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is the time-series data stationary?\n",
    "Most time-series models assumes the data is stationary (constant mean, variance, autocovariance). Our data's rolling mean appears to change with time. We can use the Dickey-Fuller test to check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationarity(df, ts):\n",
    "    \"\"\"\n",
    "    Test stationarity using moving average statistics and Dickey-Fuller test\n",
    "    Source: https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n",
    "    \"\"\"\n",
    "    # Determing rolling statistics\n",
    "    rolmean = df[ts].rolling(window=12, center=False).mean()\n",
    "    rolstd = df[ts].rolling(window=12, center=False).std()\n",
    "    \n",
    "    # Plot rolling statistics:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    orig = plt.plot(df[ts],\n",
    "                   color = 'blue',\n",
    "                   label = 'Original')\n",
    "    mean = plt.plot(rolmean,\n",
    "                   color = 'red',\n",
    "                   label = 'Rolling Mean')\n",
    "    std = plt.plot(rolstd,\n",
    "                  color = 'black',\n",
    "                  label = 'Rolling Std')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.title('Rolling Mean & Standard Deviation for %s' %(ts))\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show(block = False)\n",
    "    plt.close()\n",
    "    \n",
    "    # Perform Dickey-Fuller test:\n",
    "    # Null Hypothesis (H_0): time series is not stationary\n",
    "    # Alternate Hypothesis (H_1): time series is stationary\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(df[ts],\n",
    "                     autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4],\n",
    "                         index = ['Test Statistic',\n",
    "                                 'p-value',\n",
    "                                 '# Lags Used',\n",
    "                                 'Number of Observations Used'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' %key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(df=data_chats, ts='chats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the chart, we can see the rolling mean is increasing over time while the rolling standard deviation appears to vary a bit. Because the rolling mean is not constant, this would imply the data is not stationary. Furthermore, because the test statistic is more than the critical values we cannot reject the null hypothesis ergo we can say the time-series is not stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correcting data for stationarity\n",
    "The trend (non-constant rolling mean) and seasonality (non-constant rolling variance) need to be corrected.\n",
    "## Transforming, Smoothing, and Differencing the data\n",
    "### Transforming\n",
    "We can choose log, square root, cube root, etc to transform the data. Some will be better at smoothing than others.\n",
    "### Smoothing\n",
    "We can smooth (take rolling averages) over weekly or monthly averages. Because of the weekly cycle spotted earlier, I will smooth over a weekly average.\n",
    "### Differencing\n",
    "Will use first-order differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine what transforming, smoothing, and differencing options yield the best results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename chats to ts\n",
    "data_chats = data_chats.rename(columns={'chats':'ts'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformed_data(df, ts, ts_transform):\n",
    "  \"\"\"\n",
    "  Plot transformed and original time series data\n",
    "  Source: Tamara Louie, PyData LA, October 2018, https://github.com/tklouie/PyData_LA_2018\n",
    "  \"\"\"\n",
    "  # Plot time series data\n",
    "  f, ax = plt.subplots(1,1)\n",
    "  ax.plot(df[ts])\n",
    "  ax.plot(df[ts_transform], color = 'red')\n",
    "\n",
    "  # Add title\n",
    "  ax.set_title('%s and %s time-series graph' %(ts, ts_transform))\n",
    "\n",
    "  # Rotate x-labels\n",
    "  ax.tick_params(axis = 'x', rotation = 45)\n",
    "\n",
    "  # Add legend\n",
    "  ax.legend([ts, ts_transform])\n",
    "  \n",
    "  plt.show()\n",
    "  plt.close()\n",
    "  \n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation - log ts\n",
    "data_chats['ts_log'] = data_chats['ts'].apply(lambda x: np.log(x))\n",
    "\n",
    "# Transformation - 7-day moving averages of log ts\n",
    "data_chats['ts_log_moving_avg'] = data_chats['ts_log'].rolling(window = 7,\n",
    "                                                               center = False).mean()\n",
    "\n",
    "# Transformation - 7-day moving average ts\n",
    "data_chats['ts_moving_avg'] = data_chats['ts'].rolling(window = 7,\n",
    "                                                       center = False).mean()\n",
    "\n",
    "# Transformation - Difference between logged ts and first-order difference logged ts\n",
    "# data_chats['ts_log_diff'] = data_chats['ts_log'] - data_chats['ts_log'].shift()\n",
    "data_chats['ts_log_diff'] = data_chats['ts_log'].diff()\n",
    "\n",
    "# Transformation - Difference between ts and moving average ts\n",
    "data_chats['ts_moving_avg_diff'] = data_chats['ts'] - data_chats['ts_moving_avg']\n",
    "\n",
    "# Transformation - Difference between logged ts and logged moving average ts\n",
    "data_chats['ts_log_moving_avg_diff'] = data_chats['ts_log'] - data_chats['ts_log_moving_avg']\n",
    "\n",
    "# Transformation - Difference between logged ts and logged moving average ts\n",
    "data_chats_transform = data_chats.dropna()\n",
    "\n",
    "# Transformation - Logged exponentially weighted moving averages (EWMA) ts\n",
    "data_chats_transform['ts_log_ewma'] = data_chats_transform['ts_log'].ewm(halflife = 7,\n",
    "                                                                         ignore_na = False,\n",
    "                                                                         min_periods = 0,\n",
    "                                                                         adjust = True).mean()\n",
    "\n",
    "# Transformation - Difference between logged ts and logged EWMA ts\n",
    "data_chats_transform['ts_log_ewma_diff'] = data_chats_transform['ts_log'] - data_chats_transform['ts_log_ewma']\n",
    "\n",
    "# Display data\n",
    "display(data_chats_transform.head())\n",
    "\n",
    "# Plot data\n",
    "plot_transformed_data(df = data_chats, \n",
    "                      ts = 'ts', \n",
    "                      ts_transform = 'ts_log')\n",
    "# Plot data\n",
    "plot_transformed_data(df = data_chats, \n",
    "                      ts = 'ts_log', \n",
    "                      ts_transform = 'ts_log_moving_avg')\n",
    "\n",
    "# Plot data\n",
    "plot_transformed_data(df = data_chats_transform, \n",
    "                      ts = 'ts', \n",
    "                      ts_transform = 'ts_moving_avg')\n",
    "\n",
    "# Plot data\n",
    "plot_transformed_data(df = data_chats_transform, \n",
    "                      ts = 'ts_log', \n",
    "                      ts_transform = 'ts_log_diff')\n",
    "\n",
    "# Plot data\n",
    "plot_transformed_data(df = data_chats_transform, \n",
    "                      ts = 'ts', \n",
    "                      ts_transform = 'ts_moving_avg_diff')\n",
    "\n",
    "# Plot data\n",
    "plot_transformed_data(df = data_chats_transform, \n",
    "                      ts = 'ts_log', \n",
    "                      ts_transform = 'ts_log_moving_avg_diff')\n",
    "\n",
    "# Plot data\n",
    "plot_transformed_data(df = data_chats_transform, \n",
    "                      ts = 'ts_log', \n",
    "                      ts_transform = 'ts_log_ewma')\n",
    "\n",
    "# Plot data\n",
    "plot_transformed_data(df = data_chats_transform, \n",
    "                      ts = 'ts_log', \n",
    "                      ts_transform = 'ts_log_ewma_diff')\n",
    "\n",
    "# Perform stationarity test\n",
    "test_stationarity(df = data_chats_transform, \n",
    "                  ts = 'ts_log')\n",
    "\n",
    "# Perform stationarity test\n",
    "test_stationarity(df = data_chats_transform, \n",
    "                  ts = 'ts_moving_avg')\n",
    "\n",
    "# Perform stationarity test\n",
    "test_stationarity(df = data_chats_transform, \n",
    "                  ts = 'ts_log_moving_avg')\n",
    "\n",
    "# Perform stationarity test\n",
    "test_stationarity(df = data_chats_transform,\n",
    "                  ts = 'ts_log_diff')\n",
    "\n",
    "# Perform stationarity test\n",
    "test_stationarity(df = data_chats_transform,\n",
    "                  ts = 'ts_moving_avg_diff')\n",
    "\n",
    "# Perform stationarity test\n",
    "test_stationarity(df = data_chats_transform,\n",
    "                  ts = 'ts_log_moving_avg_diff')\n",
    "\n",
    "# Perform stationarity test\n",
    "test_stationarity(df = data_chats_transform, \n",
    "                  ts = 'ts_log_ewma')\n",
    "\n",
    "# Perform stationarity test\n",
    "test_stationarity(df = data_chats_transform,\n",
    "                  ts = 'ts_log_ewma_diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best option in making the data more stationary with time is to apply a log transformation, weekly moving average smoothing, and differencing. This can be concluded by examining the p-value in each of the Dickey-Fuller tests. Because the test statistic is smaller than the 1% critical values, we can say with 99% confidence that this is a stationary series.\n",
    "\n",
    "However, even with a low p-value I am a little worried about the variance as it appears to decrease over time (ergo, not constant). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposing\n",
    "\n",
    "Let us go a little further and decompose the time-series data into its trend, seasonality, and residual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decomposition = seasonal_decompose(data_chats_transform['ts'])\n",
    "data_chats_transform.loc[:,'trend'] = decomposition.trend\n",
    "data_chats_transform.loc[:,'seasonal'] = decomposition.seasonal\n",
    "data_chats_transform.loc[:,'residual'] = decomposition.resid\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(411)\n",
    "plt.plot(data_chats_transform['ts'], label='Original')\n",
    "plt.legend()\n",
    "plt.subplot(412)\n",
    "plt.plot(data_chats_transform['trend'], label='Trend')\n",
    "plt.legend()\n",
    "plt.subplot(413)\n",
    "plt.plot(data_chats_transform['seasonal'],label='Seasonality')\n",
    "plt.legend()\n",
    "plt.subplot(414)\n",
    "plt.plot(data_chats_transform['residual'], label='Residuals')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Dickey-Fuller test on residuals\n",
    "test_stationarity(df = data_chats_transform.dropna(), ts = 'residual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the trend and seasonality makes the residual data stationary. The test statistic is much lower than the 1% critical value. As such the time-series is considered stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "## ARIMA Model\n",
    "ARIMA 'p' and 'q' values will be determined with the help of ACF and PACF plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACF and PACF plots\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "lag_acf = acf(np.array(data_chats_transform['ts_log_moving_avg_diff']), nlags=20)\n",
    "lag_pacf = pacf(np.array(data_chats_transform['ts_log_moving_avg_diff']), nlags=20, method='ols')\n",
    "\n",
    "#ACF\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(data_chats['ts'])),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(data_chats['ts'])),linestyle='--',color='gray')\n",
    "plt.title('Autocorrelation Function')\n",
    "\n",
    "#Plot PACF:\n",
    "plt.subplot(122)\n",
    "plt.plot(lag_pacf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(data_chats['ts'])),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(data_chats['ts'])),linestyle='--',color='gray')\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see it cross the upper confidence line at 1 on both plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plt.subplot(121); plot_acf(data_chats_transform['ts_moving_avg_diff'], lags = 50, ax = plt.gca())\n",
    "plt.subplot(122); plot_pacf(data_chats_transform['ts_moving_avg_diff'], lags = 50, ax = plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the autocorrelation, we can spot heteroscedasticity. From here we could:\n",
    "1. Correct for this and then run a __SARIMA__ model to take into account the weekly seasonality\n",
    "2. Run a __Prophet__ model which will correct the heteroscedasticity and specializes in working with seasonal data\n",
    "\n",
    "I am going to go with number 2 and create a __Prophet__ model as it will reduce the risk of human error and is the quicker option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "import datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_between(d1, d2):\n",
    "    \"\"\"Calculate the number of days between d1 (inclusive) and d2 (inclusive)\"\"\"\n",
    "    d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n",
    "    d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n",
    "    return abs((d2 - d1).days + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using the Prophet inputs page created by Tamara Louie, PyData LA, October 2018 to simplify the process and reduce the risk of missing an input (Prophet has a ton of inputs).\n",
    "Source: https://github.com/tklouie/PyData_LA_2018\n",
    "\"\"\"\n",
    "# Inputs for query\n",
    "\n",
    "date_column = 'dt'\n",
    "metric_column = 'ts'\n",
    "table = data_chats\n",
    "start_training_date = '2018-01-01'\n",
    "end_training_date = '2018-06-30'\n",
    "start_forecasting_date = '2018-07-01'\n",
    "end_forecasting_date = '2018-07-31'\n",
    "#year_to_estimate = '2018'\n",
    "\n",
    "# Inputs for forecasting\n",
    "\n",
    "# future_num_points\n",
    "# If doing different time intervals, change future_num_points\n",
    "future_num_points = days_between(start_forecasting_date, end_forecasting_date)\n",
    "\n",
    "cap = None # 2e6\n",
    "\n",
    "# growth: default = 'linear'\n",
    "# Can also choose 'logistic'\n",
    "growth = 'linear'\n",
    "\n",
    "# n_changepoints: default = 25, uniformly placed in first 80% of time series\n",
    "n_changepoints = 25 \n",
    "\n",
    "# changepoint_prior_scale: default = 0.05\n",
    "# Increasing it will make the trend more flexible\n",
    "changepoint_prior_scale = 0.05 \n",
    "\n",
    "# changpoints: example = ['2016-01-01']\n",
    "changepoints = None \n",
    "\n",
    "# holidays_prior_scale: default = 10\n",
    "# If you find that the holidays are overfitting, you can adjust their prior scale to smooth them\n",
    "holidays_prior_scale = 10 \n",
    "\n",
    "# interval_width: default = 0.8\n",
    "interval_width = 0.8 \n",
    "\n",
    "# mcmc_samples: default = 0\n",
    "# By default Prophet will only return uncertainty in the trend and observation noise.\n",
    "# To get uncertainty in seasonality, you must do full Bayesian sampling. \n",
    "# Replaces typical MAP estimation with MCMC sampling, and takes MUCH LONGER - e.g., 10 minutes instead of 10 seconds.\n",
    "# If you do full sampling, then you will see the uncertainty in seasonal components when you plot:\n",
    "mcmc_samples = 0\n",
    "\n",
    "# holiday: default = None\n",
    "# thanksgiving = pd.DataFrame({\n",
    "#   'holiday': 'thanksgiving',\n",
    "#   'ds': pd.to_datetime(['2014-11-27', '2015-11-26',\n",
    "#                         '2016-11-24', '2017-11-23']),\n",
    "#   'lower_window': 0,\n",
    "#   'upper_window': 4,\n",
    "# })\n",
    "# christmas = pd.DataFrame({\n",
    "#   'holiday': 'christmas',\n",
    "#   'ds': pd.to_datetime(['2014-12-25', '2015-12-25', \n",
    "#                         '2016-12-25','2017-12-25']),\n",
    "#   'lower_window': -1,\n",
    "#   'upper_window': 0,\n",
    "# })\n",
    "# holidays = pd.concat((thanksgiving,christmas))\n",
    "holidays = None\n",
    "\n",
    "daily_seasonality = True\n",
    "#weekly_seasonality = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust df to meet Prophet's column naming standard\n",
    "# Data\n",
    "df_prophet = data_chats[['ts']]\n",
    "\n",
    "# reset index\n",
    "df_prophet = df_prophet.reset_index()\n",
    "\n",
    "# rename columns\n",
    "df_prophet = df_prophet.rename(columns = {'date': 'ds', 'ts': 'y'})\n",
    "\n",
    "# Change 'ds' type from datetime to date (necessary for FB Prophet)\n",
    "df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
    "\n",
    "# Change 'y' type to numeric (necessary for FB Prophet)\n",
    "df_prophet['y'] = pd.to_numeric(df_prophet['y'], errors='ignore')\n",
    "\n",
    "# Remove any outliers\n",
    "# df.loc[(df_['ds'] > '2016-12-13') & (df_['ds'] < '2016-12-19'), 'y'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_daily_forecast(df,\n",
    "#                           cap,\n",
    "                          holidays,\n",
    "                          growth,\n",
    "                          n_changepoints = 25,\n",
    "                          changepoint_prior_scale = 0.05,\n",
    "                          changepoints = None,\n",
    "                          holidays_prior_scale = 10,\n",
    "                          interval_width = 0.8,\n",
    "                          mcmc_samples = 1,\n",
    "                          future_num_points = future_num_points, \n",
    "                          daily_seasonality = True,\n",
    "#                          weekly_seasonality = True\n",
    "                         ):\n",
    "  \"\"\"\n",
    "  Create forecast\n",
    "  \"\"\"\n",
    "  \n",
    "  # Create copy of dataframe\n",
    "  df_ = df.copy()\n",
    "\n",
    "  # Add in growth parameter, which can change over time\n",
    "  #     df_['cap'] = max(df_['y']) if cap is None else cap\n",
    "\n",
    "  # Create model object and fit to dataframe\n",
    "  m = Prophet(growth = growth,\n",
    "              n_changepoints = n_changepoints,\n",
    "              changepoint_prior_scale = changepoint_prior_scale,\n",
    "              changepoints = changepoints,\n",
    "              holidays = holidays,\n",
    "              holidays_prior_scale = holidays_prior_scale,\n",
    "              interval_width = interval_width,\n",
    "              mcmc_samples = mcmc_samples, \n",
    "              daily_seasonality = daily_seasonality,\n",
    "#              weekly_seasonality = weekly_seasonality\n",
    "             )\n",
    "\n",
    "  # Fit model with dataframe\n",
    "  m.fit(df_)\n",
    "\n",
    "  # Create dataframe for predictions\n",
    "  future = m.make_future_dataframe(periods = future_num_points)\n",
    "  #     future['cap'] = max(df_['y']) if cap is None else cap\n",
    "\n",
    "  # Create predictions\n",
    "  fcst = m.predict(future)\n",
    "\n",
    "  # Plot\n",
    "  m.plot(fcst);\n",
    "  m.plot_components(fcst)\n",
    "\n",
    "  return fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-register pd converters\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "# Forecast data\n",
    "fcst = create_daily_forecast(df_prophet,\n",
    "#                              cap,\n",
    "                             holidays,\n",
    "                             growth,\n",
    "                             n_changepoints,\n",
    "                             changepoint_prior_scale,\n",
    "                             changepoints, \n",
    "                             holidays_prior_scale,\n",
    "                             interval_width,\n",
    "                             mcmc_samples,\n",
    "                             future_num_points, \n",
    "                             daily_seasonality,\n",
    "#                             weekly_seasonality\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the predicted values for the month of July."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_july = fcst[['ds', 'yhat']].set_index('ds')\n",
    "predict_july = predict_july['2018-07-01':]\n",
    "predict_july.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_july.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "sns.lineplot(data=predict_july)\n",
    "plt.title(\"Predicted Chats in July 2018\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Chats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Recommendation on the number of nurses needed per day in July\n",
    "If we look at the predicted values above, on the slowest day we have a minimum of 359 chats and on the busiest day we have a max of 434 chats. There is an average of 409 chats per day in July.\n",
    "Assuming that:\n",
    "1. Nurses work in 8-hour shifts, we have 3 shifts of nurses in a day.\n",
    "2. Calls are spread out evenly throughout the day (we won't have all 400+ calls during the same hour). This is likley not true but lets use it in this example.\n",
    "3. The average duration of a chat is 30 minutes\n",
    "\n",
    "Using 409 chats, the average number of chats in July, spread evenly over the course of a day results in 17 chats per hour. With the chat duration assumption of 30 minutes, the minium number of nurses required to meet this demand is 9 nurses per hour. With 8-hour shifts, we will __need 27 nurses per day in July to handle the average number of chats during the month__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Evaluate the Accuracy of the Model\n",
    "In order to test our forecast, I will train the model on data from 2018-01-01 to 2018-05-31 and test it against June's actual values. This means the model will be trained on 151 of the 181 records (83%). The test data will be the last month which has 30 of the 181 records (17%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data_chats.loc[:'2018-05-31']\n",
    "df_test = data_chats.loc['2018-06-01':]\n",
    "\n",
    "# Check data was divided correctly\n",
    "print(df_train['ts'].count(), df_test['ts'].count(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the inputs for the Prophet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for query\n",
    "date_column = 'dt'\n",
    "metric_column = 'ts'\n",
    "table = df_train\n",
    "start_training_date = '2018-01-01'\n",
    "end_training_date = '2018-05-31'\n",
    "start_forecasting_date = '2018-06-01'\n",
    "end_forecasting_date = '2018-06-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust df to meet Prophet's column naming standard\n",
    "# Data\n",
    "df_prophet = df_train[['ts']]\n",
    "\n",
    "# reset index\n",
    "df_prophet = df_prophet.reset_index()\n",
    "\n",
    "# rename columns\n",
    "df_prophet = df_prophet.rename(columns = {'date': 'ds', 'ts': 'y'})\n",
    "\n",
    "# Change 'ds' type from datetime to date (necessary for FB Prophet)\n",
    "df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
    "\n",
    "# Change 'y' type to numeric (necessary for FB Prophet)\n",
    "df_prophet['y'] = pd.to_numeric(df_prophet['y'], errors='ignore')\n",
    "\n",
    "# Remove any outliers\n",
    "# df.loc[(df_['ds'] > '2016-12-13') & (df_['ds'] < '2016-12-19'), 'y'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-register pd converters\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "# Forecast data\n",
    "fcst = create_daily_forecast(df_prophet,\n",
    "#                              cap,\n",
    "                             holidays,\n",
    "                             growth,\n",
    "                             n_changepoints,\n",
    "                             changepoint_prior_scale,\n",
    "                             changepoints, \n",
    "                             holidays_prior_scale,\n",
    "                             interval_width,\n",
    "                             mcmc_samples,\n",
    "                             future_num_points, \n",
    "                             daily_seasonality,\n",
    "#                             weekly_seasonality\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the accuracy of the model, we have already divided the data into training data and testing data using a 83% train and 17% test split. We will now combine the forecast values in the fcst dataframe with the actual values and compare them by calculating the MAPE and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = fcst.set_index('ds')[['yhat', 'yhat_lower', 'yhat_upper']].join(data_chats['ts'])\n",
    "df_compare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to use MAPE and MAE to measure quality of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fcst_errors(df, fcst_size):\n",
    "    \"\"\" Calculates MAPE and MAE of the Prophet forecast.\n",
    "    Uses df_compare dataframe (yhat, yhat_lower, yhat_upper, and ts columns) and fcst_size (default argument is the future_num_points input from Prophet's inputs)\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['e'] = df['ts'] - df['yhat']\n",
    "    df['p'] = 100 * df['e'] / df['ts']\n",
    "    \n",
    "    # Separate out the predicted values in the forecast\n",
    "    predict_values = df[-fcst_size:]\n",
    "    \n",
    "    # Predicted average absolute error\n",
    "    error_mean = lambda err: np.mean(np.abs(predict_values[err]))\n",
    "    \n",
    "    # Calculate MAPE & MAE\n",
    "    return {'MAPE': error_mean('p'), 'MAE': error_mean('e')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for err_name, err_value in calc_fcst_errors(df = df_compare, fcst_size = future_num_points).items():\n",
    "    print(err_name, err_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAPE of the model, or the relative error, is about 12.4%. The MAE states that the model is wrong by 49.4 predicts on average.\n",
    "\n",
    "Let us take a look at the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly import graph_objs as go\n",
    "\n",
    "# Initialize plotly\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_forecast(df_compare, future_num_points, num_values):\n",
    "    def create_go(name, column, num, **kwargs):\n",
    "        points = df_compare.tail(num)\n",
    "        args = dict(name=name, x=points.index, y=points[column], mode='lines')\n",
    "        args.update(kwargs)\n",
    "        return go.Scatter(**args)\n",
    "    \n",
    "    lower_bound = create_go('Lower Bound', 'yhat_lower', future_num_points,\n",
    "                            line=dict(width=0),\n",
    "                            marker=dict(color=\"aqua\"))\n",
    "    upper_bound = create_go('Upper Bound', 'yhat_upper', future_num_points,\n",
    "                            line=dict(width=0),\n",
    "                            marker=dict(color=\"aqua\"),\n",
    "                            fillcolor='rgba(68, 68, 68, 0.3)', \n",
    "                            fill='tonexty')\n",
    "    forecast = create_go('Forecast', 'yhat', future_num_points,\n",
    "                         line=dict(color='rgb(31, 119, 180)'))\n",
    "    actual = create_go('Actual', 'ts', num_values,\n",
    "                       marker=dict(color=\"red\"))\n",
    "    \n",
    "    data = [lower_bound, upper_bound, forecast, actual]\n",
    "\n",
    "    layout = go.Layout(yaxis=dict(title='Chats'), title='Daily Chats', showlegend = False)\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_compare' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e300ee6ade1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshow_forecast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_compare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfuture_num_points\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m181\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_compare' is not defined"
     ]
    }
   ],
   "source": [
    "show_forecast(df_compare, future_num_points, 181)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, we can see the model is maintaing the upwards trend based on the training data from January to the end of May. In June, the actual values experienced a decrease in the trend. Overall, the model is a good start with the limited amount of data available. In a few years we might even see a yearly seasonality to the data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: How to evaluate the impact of the model\n",
    "If the team starts using the model to staff the clinic, we can begin to evaluate the impact of the model by measuring and comparing the user wait times before they're connected to a nurse. If the wait times are too long, it would imply we are short on nurses which degrades the quality of the service. If the average wait time is near zero, it would imply we have more active nurses than necessary which indicates that our costs are potentially higher than necessary.\n",
    "\n",
    "If the wait time is at the target value, it would indicate that the model is accurately predicting the number of chats and therefore the number of active nurses required.\n",
    "\n",
    "Note that we can also collect and compare July's actual values to it's predicted values to measure its accuracy.\n",
    "\n",
    "Also note that the model's accuracy will degrade the further out you forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: Improving the model's accuracy and its impact on cost and quality of service\n",
    "To improve the model's accuracy and its impact on quality of service we can:\n",
    "1. Run the prophet model with transformed data. For example, we could run the Prophet model with the log of our time-series, or the log and difference.\n",
    "2. Optimize the inputs of our Prophet model. We could evaluate with a logistic growth or add in holidays to see if they improve the accuracy of the model.\n",
    "3. Create a SARIMA model, optimizing its 'p', 'q', and 'd' values, to see if it will produce a model with better accuracy.\n",
    "4. Explore different methods of improving the stationarity of the time-series data.\n",
    "5. Explore other types of models such as recurrent neural networks or long short-term memory networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
